{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting the Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BibW7EMun1l_",
    "outputId": "db44da80-0292-4bbe-b641-83a9368285f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PzIcOTGIoQ43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/Ml_imp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VvBUhV7Bl0C"
   },
   "source": [
    "### 1.) Data Preprocessing Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D65-8r6xoQ2L"
   },
   "outputs": [],
   "source": [
    "# Importing Essential Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "COTxjhnHoQz1"
   },
   "outputs": [],
   "source": [
    "# Importing The Train & Test Dataset\n",
    "\n",
    "data_train=pd.read_csv('/content/drive/MyDrive/Ml_imp/Train.csv')\n",
    "data_test=pd.read_csv('/content/drive/MyDrive/Ml_imp/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TahksswZCKnR"
   },
   "outputs": [],
   "source": [
    "# Removing the Id & Delivery_person_id as they are of no use w.r.t prediction\n",
    "\n",
    "X_test=data_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mhw1dFzHoQxd"
   },
   "outputs": [],
   "source": [
    "# Dropping Redundant & Date Time Columns as they are not contributing that much Variance \n",
    "\n",
    "X_test=X_test.drop('Order_Date', axis=1)\n",
    "X_test=X_test.drop('Time_Orderd', axis=1)\n",
    "X_test=X_test.drop('Time_Order_picked', axis=1)\n",
    "# Dropping Restaurant latitude & logitude as they were the lowest K best Features and also had multicollinearity>0.4\n",
    "\n",
    "X_test=X_test.drop('Restaurant_latitude',axis=1)\n",
    "X_test=X_test.drop('Restaurant_longitude',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fHi4Wx-ToQun"
   },
   "outputs": [],
   "source": [
    "# Extracting the ID's of Test Dataset and will be used to combine with the predcited Y_test\n",
    "\n",
    "I=data_test.iloc[:,0]\n",
    "I=list(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZYOzgE5pUMZ",
    "outputId": "68f869de-f1f6-4714-ebc8-e13acc520396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery_person_Age            4.91\n",
      "Delivery_person_Ratings        5.07\n",
      "Delivery_location_latitude     0.00\n",
      "Delivery_location_longitude    0.00\n",
      "Weather conditions             1.58\n",
      "Road_traffic_density           1.54\n",
      "Vehicle_condition              0.00\n",
      "Type_of_order                  0.00\n",
      "Type_of_vehicle                0.00\n",
      "multiple_deliveries            2.38\n",
      "Festival                       0.65\n",
      "City                           3.24\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Finding the Missing Values\n",
    "\n",
    "missing_percentage = X_test.isnull().sum()/100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p27S6wEApXlP"
   },
   "outputs": [],
   "source": [
    "# Filling Missing values of Numerical Columns with mean & Categorical Column by Mode\n",
    "\n",
    "X_test['Delivery_person_Age'] = X_test['Delivery_person_Age'].fillna(X_test['Delivery_person_Age'].mean())\n",
    "X_test['Delivery_person_Ratings'] = X_test['Delivery_person_Ratings'].fillna(X_test['Delivery_person_Ratings'].mean())\n",
    "X_test['multiple_deliveries'] = X_test['multiple_deliveries'].fillna(X_test['multiple_deliveries'].mean())\n",
    "X_test['Weather conditions']=X_test['Weather conditions'].fillna(X_test['Weather conditions'].mode()[0])\n",
    "X_test['Road_traffic_density']=X_test['Road_traffic_density'].fillna(X_test['Road_traffic_density'].mode()[0])\n",
    "X_test['Festival']=X_test['Festival'].fillna(X_test['Festival'].mode()[0])\n",
    "X_test['City']=X_test['City'].fillna(X_test['City'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xDJuSzQ3pbnd"
   },
   "outputs": [],
   "source": [
    "# Binary Class: Encoding the Festival Column into 0,1\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_test.iloc[:,10] = le.fit_transform(X_test.iloc[:, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dCJW21Bupbk2"
   },
   "outputs": [],
   "source": [
    "# Multi Class: Encoding these columns by One Hot Encoding \n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [4,5,7,8,11])], remainder='passthrough')\n",
    "X_test = np.array(ct.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adZH7yTwpbev",
    "outputId": "25c978d5-8e31-4f13-c560-399c078796bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11399, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QXZmJxhepqyL"
   },
   "outputs": [],
   "source": [
    "# Splitting the Data into X(Independant) & Y(Dependant)-- Target Variables\n",
    "X=data_train.iloc[:,2:-1]\n",
    "Y=data_train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VmYPNlW_ptUd"
   },
   "outputs": [],
   "source": [
    "# Dropping Redundant & Date Time Columns as they are not contributing that much Variance \n",
    "\n",
    "X=X.drop('Order_Date', axis=1)\n",
    "X=X.drop('Time_Orderd', axis=1)\n",
    "X=X.drop('Time_Order_picked', axis=1)\n",
    "# Dropping Restaurant latitude & logitude as they were the lowest K best Features and also had multicollinearity>0.4\n",
    "X=X.drop('Restaurant_latitude',axis=1)\n",
    "X=X.drop('Restaurant_longitude',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SI9J3pOEXi5",
    "outputId": "424f9cf1-2651-458a-c574-167e0f3c02df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery_person_Age            18.54\n",
      "Delivery_person_Ratings        19.08\n",
      "Delivery_location_latitude      0.00\n",
      "Delivery_location_longitude     0.00\n",
      "Weather conditions              6.16\n",
      "Road_traffic_density            6.01\n",
      "Vehicle_condition               0.00\n",
      "Type_of_order                   0.00\n",
      "Type_of_vehicle                 0.00\n",
      "multiple_deliveries             9.93\n",
      "Festival                        2.28\n",
      "City                           12.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Finding the Missing Values\n",
    "\n",
    "missing_percentage = X.isnull().sum()/100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UTKtHmmFpu04"
   },
   "outputs": [],
   "source": [
    "# Filling Missing values of Numerical Columns with mean & Categorical Column by Mode\n",
    "\n",
    "X['Delivery_person_Age'] = X['Delivery_person_Age'].fillna(X['Delivery_person_Age'].mean())\n",
    "X['Delivery_person_Ratings'] = X['Delivery_person_Ratings'].fillna(X['Delivery_person_Ratings'].mean())\n",
    "X['multiple_deliveries'] = X['multiple_deliveries'].fillna(X['multiple_deliveries'].mean())\n",
    "X['Weather conditions']=X['Weather conditions'].fillna(X['Weather conditions'].mode()[0])\n",
    "X['Road_traffic_density']=X['Road_traffic_density'].fillna(X['Road_traffic_density'].mode()[0])\n",
    "X['Festival']=X['Festival'].fillna(X['Festival'].mode()[0])\n",
    "X['City']=X['City'].fillna(X['City'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_3ILO0tfpzQo"
   },
   "outputs": [],
   "source": [
    "# Binary Class: Encoding the Festival Column into 0,1\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X.iloc[:,10] = le.fit_transform(X.iloc[:, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8BFXbJJxp10n"
   },
   "outputs": [],
   "source": [
    "# Multi Class: Encoding these columns by One Hot Encoding \n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [4,5,7,8,11])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mA8Lj2iEp32c"
   },
   "outputs": [],
   "source": [
    "# Scaling the X & X_test values by Standard_Scaler: Mean=0, Standard Deviation=1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X= sc_X.fit_transform(X)\n",
    "\n",
    "# Only Transform on X_test as we do not want to change the values of X_test as it will then give wrong & bad Y_pred Results on Y_test\n",
    "\n",
    "X_test=sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPxgQPN6s2kp",
    "outputId": "d01b3c65-7d6f-46d4-b242-ac84313e1062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276604"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hifdicJjFZ__"
   },
   "source": [
    "### 3. Machine Learning Algorithm Selection:\n",
    "Supervised ML - Multiple Regression, Decision Trees, Support Vector Machines, Random Forest Regressor, XG Boost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvoxzD4KFwMF"
   },
   "source": [
    "### 4. Model Building, Evaluation & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNqHT360F5ef"
   },
   "outputs": [],
   "source": [
    "# 1.) Multiple Regression\n",
    "\n",
    "# Importing LinearRegression from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating Regressor Object\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# Fitting the Regressor on X,Y\n",
    "regressor.fit(X,Y)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO4tQ4_1F5al"
   },
   "outputs": [],
   "source": [
    "# Convert numpy array results into List\n",
    "\n",
    "y_pred=list(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhjshRdOGxmE"
   },
   "source": [
    "#### Sample_Submission_File: 11399 X 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Nivd_2vGa09"
   },
   "outputs": [],
   "source": [
    "# Concat the Test Id's with Y_pred\n",
    "\n",
    "df=pd.DataFrame(list(zip(I,y_pred)))\n",
    "df.columns=['ID','Time_taken (min)']\n",
    "df.to_csv(\"MR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPnLFNZYF5XF"
   },
   "outputs": [],
   "source": [
    "# Doing K-Cross Validation to reduce Overfitting & get lowest Mse Value\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mse=cross_val_score(regressor,X,Y,scoring='neg_mean_squared_error',cv=10)\n",
    "np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAmsMz_7HXdp"
   },
   "outputs": [],
   "source": [
    "# 2.) Ridge Regression\n",
    "\n",
    "# Importing Ridge & GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ridge_regressor=Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2TrepDEHsDV"
   },
   "source": [
    "#### -- Hyper Parameter Tuning -  Grid Search Cv -- ON Alpha as the only Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4TViLodHjV4"
   },
   "outputs": [],
   "source": [
    "# Defing K:v Pair of Alpha\n",
    "parameters={'alpha':[1,2,5,10,20,30,40,50,60,70,80,90,100]}\n",
    "\n",
    "# Using Grid Search Cv with Cv=10 cross-validations\n",
    "ridgecv=GridSearchCV(ridge_regressor,parameters,scoring='neg_mean_squared_error',cv=10)\n",
    "ridgecv.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xxx72oPrH1XC"
   },
   "outputs": [],
   "source": [
    "print(\"The Best Parameter value is:{}, Best Score is:{}\".format(ridgecv.best_params_,ridgecv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avQaqJEyH5WB"
   },
   "outputs": [],
   "source": [
    "y_pred_2=ridgecv.predict(X_test)\n",
    "y_pred_2 = list(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXqXoqDLIKbJ"
   },
   "outputs": [],
   "source": [
    "# Concat the Test Id's with Y_pred\n",
    "\n",
    "df=pd.DataFrame(list(zip(I,y_pred_2)))\n",
    "df.columns=['ID','Time_taken (min)']\n",
    "df.to_csv(\"Ridge_R.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUZN6qiQIQKf"
   },
   "outputs": [],
   "source": [
    "# 3.) Lasso Regression\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_regressor=Lasso()\n",
    "parameters={'alpha':[1,2,5,10,20,30,40,50,60,70,80,90,100]}\n",
    "lassocv=GridSearchCV(lasso_regressor,parameters,scoring='neg_mean_squared_error',cv=10)\n",
    "lassocv.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YXA2rqyIP57"
   },
   "outputs": [],
   "source": [
    "print(\"The Best Parameter value is:{}, Best Score is:{}\".format(lassocv.best_params_,lassocv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGQmaX1lIP3J"
   },
   "outputs": [],
   "source": [
    "y_pred_3=lassocv.predict(X_test)\n",
    "y_pred_3 = list(y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ChHVniQIPzu"
   },
   "outputs": [],
   "source": [
    "# Concat the Test Id's with Y_pred\n",
    "\n",
    "df=pd.DataFrame(list(zip(I,y_pred_3)))\n",
    "df.columns=['ID','Time_taken (min)']\n",
    "df.to_csv(\"Lasso_R.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMJ08jwQ2Glb",
    "outputId": "83f56a49-e44b-4311-8d5d-69a82d5dcbcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.) SVR(Support Vector Regressor)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "svm_regressor = SVR(kernel = 'rbf')\n",
    "svm_regressor.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "y9pBMvAtI0eQ"
   },
   "outputs": [],
   "source": [
    "y_pred_4=svm_regressor.predict(X_test)\n",
    "y_pred_4=list(y_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSctkc31nRxg",
    "outputId": "d668b8a3-bd4b-4a83-9ceb-50a6751778ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-30.67270637660721"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing K-Cross Validation to reduce Overfitting & get lowest Mse Value\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mse=cross_val_score(svm_regressor,X,Y,scoring='neg_mean_squared_error',cv=10)\n",
    "np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq-HLxmm6jzy",
    "outputId": "81e02cb1-8016-4ae9-e736-b66824ba504b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=28, n_estimators=500, random_state=22)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.) Random Forest Regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor_1 = RandomForestRegressor(n_estimators=500,max_features=28,random_state=22)\n",
    "regressor_1.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F14GCKI8njnR"
   },
   "outputs": [],
   "source": [
    "y_pred_5=regressor_1.predict(y_pred_5)\n",
    "y_pred_5=list(y_pred_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CDpw0iCnqPQ"
   },
   "outputs": [],
   "source": [
    "# Doing K-Cross Validation to reduce Overfitting & get lowest Mse Value\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mse=cross_val_score(regressor_1,X,Y,scoring='neg_mean_squared_error',cv=10)\n",
    "np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAra5JxSnyvr"
   },
   "outputs": [],
   "source": [
    "# Concat the Test Id's with Y_pred\n",
    "\n",
    "df=pd.DataFrame(list(zip(I,y_pred_5)))\n",
    "df.columns=['ID','Time_taken (min)']\n",
    "df.to_csv('/content/RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "8Zrt8qoU2sGD",
    "outputId": "137dce1b-8abe-45d3-c198-b4ec0684bdc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:10:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:16:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:16:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:17:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:17:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:18:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-49e48d864d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \"\"\"\n\u001b[1;32m    758\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             )\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_method_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         )\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m     )\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 )\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 6.) Stacked Random Forest Regressor & XG Boost Regressor \n",
    "\n",
    "# Ensemble 3rd Tech - where we use to combine heterogeneous Class & Regr Models.\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "estimators = [('xgb1', XGBRegressor(n_estimators=1000,eta=0.05,random_state=22)), \n",
    "               ('rf', RandomForestRegressor(n_estimators=1000,max_features=28,random_state=22)),\n",
    "             ]\n",
    "\n",
    "clf = StackingRegressor(estimators=estimators, final_estimator=XGBRegressor())\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "T4qMHlAE30Ko"
   },
   "outputs": [],
   "source": [
    "y_clf=clf.predict(X_test)\n",
    "y_clf=list(y_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "J3-MV8vn6gf8"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(list(zip(I,y_clf)))\n",
    "df.columns=['ID','Time_taken (min)']\n",
    "df.to_csv('/content/Stacked_Model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vC_jyUvv8i67",
    "outputId": "f8a1e3aa-92b6-4f67-e9d0-04d114bf65a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=0)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.) Decision Tree Regressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor_2 = DecisionTreeRegressor(random_state = 0)\n",
    "regressor_2.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqpym2JKuYe_",
    "outputId": "7228f185-b784-4579-ff30-cb0ebc8505b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45593, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_7=regressor_2.predict(X_test)\n",
    "y_pred_7=list(y_pred_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMuXi7nCpUYW"
   },
   "outputs": [],
   "source": [
    "# Doing K-Cross Validation to reduce Overfitting & get lowest Mse Value\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mse=cross_val_score(regressor_1,X,Y,scoring='neg_mean_squared_error',cv=10)\n",
    "np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diDN_KpuUuts",
    "outputId": "145cb32a-5ea1-4f51-b205-c5a12e9d21e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=DecisionTreeRegressor(random_state=0),\n",
       "                 max_features=28, n_estimators=300, random_state=22)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.) Bagged Decision Tree Regressor\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "bag_clf = BaggingRegressor(\n",
    "    DecisionTreeRegressor(random_state=0),n_estimators=300,max_features=28,random_state=22)\n",
    "bag_clf.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ejwd0KxJpijD"
   },
   "outputs": [],
   "source": [
    "y_pred_8=bag_clf.predict(X_test)\n",
    "y_pred_8=list(y_pred_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IRTHn2xpifS"
   },
   "outputs": [],
   "source": [
    "# Doing K-Cross Validation to reduce Overfitting & get lowest Mse Value\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mse=cross_val_score(bag_clf,X,Y,scoring='neg_mean_squared_error',cv=10)\n",
    "np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfJAAvhbXdvd",
    "outputId": "43253147-c99f-43a0-c54e-b12389282348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# 9.) XG Boost Regressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# printing the accuracy score\n",
    "xgb = XGBRegressor(n_estimators=300,eta=0.1,random_state=22) #max_depth and eta were set after hyperparamter tuning \n",
    "xgb.fit(X, Y) #fit model on train set\n",
    "xgb_pred_y_te = xgb.predict(X_test) #predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "A-N2uhubVwRu"
   },
   "outputs": [],
   "source": [
    "xgb_pred_y_te = list(xgb_pred_y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WG73tFAp_yG"
   },
   "outputs": [],
   "source": [
    "# Doing K-Cross Validation to reduce Overfitting & get lowest Mse Value\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mse=cross_val_score(xgb,X,Y,scoring='neg_mean_squared_error',cv=10)\n",
    "np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UyPTpxg6Qlg"
   },
   "outputs": [],
   "source": [
    "y_pred_test=list(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "hlOveAEY6i5Q"
   },
   "outputs": [],
   "source": [
    "y_pred_6=bag_clf.predict(X_test)\n",
    "y_pred_6=list(y_pred_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGR5RUsF8nJh"
   },
   "outputs": [],
   "source": [
    "y_pred_7=regressor_2.predict(X_test)\n",
    "y_pred_7=list(y_pred_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "gCm_7Jen6jCR"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(list(zip(I,y_pred_7)))\n",
    "df.columns=['ID','Time_taken (min)']\n",
    "df.to_csv('/content/RF_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "aXxoRKesv_P8"
   },
   "outputs": [],
   "source": [
    "# 10.) Random Forest Regressor with RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Randomized Search CV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 12)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [28]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n",
    "# max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "YLF090XLCf_I"
   },
   "outputs": [],
   "source": [
    "# Creating the rd Object\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "orogakcyCgfk"
   },
   "outputs": [],
   "source": [
    "# Initialization the parameters in RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=22,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuwPO9nsC1pe",
    "outputId": "9ddd12a7-7782-4627-e80f-a9d92d5de608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=5, min_samples_split=5, n_estimators=263; total time=  35.2s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=5, min_samples_split=5, n_estimators=263; total time=  35.0s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=5, min_samples_split=5, n_estimators=263; total time=  35.0s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=5, min_samples_split=2, n_estimators=427; total time=  57.5s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=5, min_samples_split=2, n_estimators=427; total time=  57.4s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=5, min_samples_split=2, n_estimators=427; total time=  57.3s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=10, min_samples_split=100, n_estimators=263; total time=  22.8s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=10, min_samples_split=100, n_estimators=263; total time=  22.7s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=10, min_samples_split=100, n_estimators=263; total time=  22.8s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=345; total time=  55.0s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=345; total time=  54.9s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=345; total time=  54.6s\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=10, min_samples_split=15, n_estimators=1000; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=10, min_samples_split=15, n_estimators=1000; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=10, min_samples_split=15, n_estimators=1000; total time= 2.0min\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=263; total time=  23.7s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=263; total time=  23.8s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=263; total time=  23.6s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=1, min_samples_split=5, n_estimators=345; total time=  31.1s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=1, min_samples_split=5, n_estimators=345; total time=  31.6s\n",
      "[CV] END max_depth=10, max_features=28, min_samples_leaf=1, min_samples_split=5, n_estimators=345; total time=  31.2s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=1, min_samples_split=15, n_estimators=100; total time=  13.3s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=1, min_samples_split=15, n_estimators=100; total time=  13.3s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=1, min_samples_split=15, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=10, min_samples_split=10, n_estimators=918; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=10, min_samples_split=10, n_estimators=918; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=10, min_samples_split=10, n_estimators=918; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=10, min_samples_split=10, n_estimators=672; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=10, min_samples_split=10, n_estimators=672; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=10, min_samples_split=10, n_estimators=672; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=5, min_samples_split=2, n_estimators=509; total time= 1.1min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=5, min_samples_split=2, n_estimators=509; total time= 1.1min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=5, min_samples_split=2, n_estimators=509; total time= 1.1min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=2, min_samples_split=10, n_estimators=754; total time= 1.7min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=2, min_samples_split=10, n_estimators=754; total time= 1.7min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=2, min_samples_split=10, n_estimators=754; total time= 1.7min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=1, min_samples_split=10, n_estimators=836; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=1, min_samples_split=10, n_estimators=836; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=1, min_samples_split=10, n_estimators=836; total time= 2.0min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=1, min_samples_split=2, n_estimators=181; total time=  31.8s\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=1, min_samples_split=2, n_estimators=181; total time=  31.6s\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=1, min_samples_split=2, n_estimators=181; total time=  31.7s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=754; total time= 1.9min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=754; total time= 1.9min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=2, min_samples_split=2, n_estimators=754; total time= 1.9min\n",
      "[CV] END max_depth=5, max_features=28, min_samples_leaf=1, min_samples_split=2, n_estimators=181; total time=   9.2s\n",
      "[CV] END max_depth=5, max_features=28, min_samples_leaf=1, min_samples_split=2, n_estimators=181; total time=   9.1s\n",
      "[CV] END max_depth=5, max_features=28, min_samples_leaf=1, min_samples_split=2, n_estimators=181; total time=   9.2s\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=1, min_samples_split=100, n_estimators=754; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=1, min_samples_split=100, n_estimators=754; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=28, min_samples_leaf=1, min_samples_split=100, n_estimators=754; total time= 1.3min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=1, min_samples_split=15, n_estimators=754; total time= 1.7min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=1, min_samples_split=15, n_estimators=754; total time= 1.7min\n",
      "[CV] END max_depth=25, max_features=28, min_samples_leaf=1, min_samples_split=15, n_estimators=754; total time= 1.7min\n",
      "[CV] END max_depth=15, max_features=28, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time= 2.0min\n",
      "[CV] END max_depth=15, max_features=28, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time= 2.0min\n",
      "[CV] END max_depth=15, max_features=28, min_samples_leaf=5, min_samples_split=5, n_estimators=1000; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=2, min_samples_split=100, n_estimators=836; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=2, min_samples_split=100, n_estimators=836; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=28, min_samples_leaf=2, min_samples_split=100, n_estimators=836; total time= 1.5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=20,\n",
       "                   param_distributions={'max_depth': [5, 10, 15, 20, 25, 30],\n",
       "                                        'max_features': [28],\n",
       "                                        'min_samples_leaf': [1, 2, 5, 10],\n",
       "                                        'min_samples_split': [2, 5, 10, 15,\n",
       "                                                              100],\n",
       "                                        'n_estimators': [100, 181, 263, 345,\n",
       "                                                         427, 509, 590, 672,\n",
       "                                                         754, 836, 918, 1000]},\n",
       "                   random_state=22, scoring='neg_mean_squared_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting on X & Y Train data\n",
    "rf_random.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8-bjhsKDMwW",
    "outputId": "5c28b47a-33d8-4257-fbd2-77e4f9599156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Parameter value is:{'n_estimators': 754, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 28, 'max_depth': 25}, Best Score is:-24.25273210556762\n"
     ]
    }
   ],
   "source": [
    "print(\"The Best Parameter value is:{}, Best Score is:{}\".format(rf_random.best_params_,rf_random.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMSUkeU-DMuF",
    "outputId": "5a49f4d6-196f-4e5a-ad23-7c5b3cc16133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.04478078, 28.80602847, 25.89672439, ..., 20.83859579,\n",
       "       23.10792044, 23.2498356 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_8=rf_random.predict(X_test)\n",
    "y_pred_8=list(y_pred_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "omZMwiCxDh9x"
   },
   "outputs": [],
   "source": [
    "df_4=pd.DataFrame(list(zip(I,y_pred_8)))\n",
    "df_4.columns=['ID','Time_taken (min)']\n",
    "df_4.to_csv('/content/Random_Forest_Tuned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0qx0-gGqhVj"
   },
   "source": [
    "### 4. Machine Learning Algorithm Selection: Deep Learning- ANN By Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U-P0xzuOFbBN"
   },
   "outputs": [],
   "source": [
    "# Importing tensorflow to build Ann\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lCLDGNgxFa25"
   },
   "outputs": [],
   "source": [
    "#Initializing the ANN\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TG1_WNg2Fayg"
   },
   "outputs": [],
   "source": [
    "# add & drop(for overfitting) is a method where keras calls the dense class \n",
    "# the dense class creates the fully connected layer where it automatically add the input layer\n",
    "# inside our Dense class Unit=no. of neurons(in dense fn we only specify how much hidden neuron's needed,activation fn=rectifier for hidden layer & sigmoid for output layer)\n",
    "\n",
    "# 3 input Layers\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wmh0DhjcFakc"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=10, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "V_0usveoHGLO"
   },
   "outputs": [],
   "source": [
    "# Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tM__HmKQHGDX"
   },
   "outputs": [],
   "source": [
    "#Compiling the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0XCmS7AHSZZ",
    "outputId": "076fa16d-c673-42df-e820-f7381227b1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 143.3451\n",
      "Epoch 2/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 41.2898\n",
      "Epoch 3/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 38.3557\n",
      "Epoch 4/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 36.8507\n",
      "Epoch 5/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 35.7123\n",
      "Epoch 6/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 34.7641\n",
      "Epoch 7/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 33.9975\n",
      "Epoch 8/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 33.5164\n",
      "Epoch 9/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 33.1580\n",
      "Epoch 10/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 32.9537\n",
      "Epoch 11/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 32.6618\n",
      "Epoch 12/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 32.4159\n",
      "Epoch 13/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 32.1479\n",
      "Epoch 14/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 31.8632\n",
      "Epoch 15/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 31.5815\n",
      "Epoch 16/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 31.4340\n",
      "Epoch 17/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 31.2079\n",
      "Epoch 18/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 31.1223\n",
      "Epoch 19/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 31.0408\n",
      "Epoch 20/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 30.8574\n",
      "Epoch 21/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 30.7839\n",
      "Epoch 22/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 30.7143\n",
      "Epoch 23/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 30.5982\n",
      "Epoch 24/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.5636\n",
      "Epoch 25/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.5087\n",
      "Epoch 26/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.3931\n",
      "Epoch 27/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.3235\n",
      "Epoch 28/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.2461\n",
      "Epoch 29/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.2228\n",
      "Epoch 30/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.1361\n",
      "Epoch 31/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.0599\n",
      "Epoch 32/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 30.0512\n",
      "Epoch 33/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.9384\n",
      "Epoch 34/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.8937\n",
      "Epoch 35/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.8696\n",
      "Epoch 36/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.8261\n",
      "Epoch 37/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.7620\n",
      "Epoch 38/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.6899\n",
      "Epoch 39/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.6643\n",
      "Epoch 40/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.6847\n",
      "Epoch 41/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.6367\n",
      "Epoch 42/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.5842\n",
      "Epoch 43/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.5536\n",
      "Epoch 44/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.5314\n",
      "Epoch 45/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.4682\n",
      "Epoch 46/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.4436\n",
      "Epoch 47/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.4130\n",
      "Epoch 48/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.3886\n",
      "Epoch 49/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 29.3299\n",
      "Epoch 50/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.2754\n",
      "Epoch 51/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.2459\n",
      "Epoch 52/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.2052\n",
      "Epoch 53/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.1325\n",
      "Epoch 54/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.1218\n",
      "Epoch 55/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.0730\n",
      "Epoch 56/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.0557\n",
      "Epoch 57/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.0131\n",
      "Epoch 58/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 29.0089\n",
      "Epoch 59/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.9863\n",
      "Epoch 60/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.9815\n",
      "Epoch 61/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.9343\n",
      "Epoch 62/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.9198\n",
      "Epoch 63/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.8949\n",
      "Epoch 64/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.8101\n",
      "Epoch 65/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.8295\n",
      "Epoch 66/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.8179\n",
      "Epoch 67/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.8278\n",
      "Epoch 68/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7955\n",
      "Epoch 69/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7808\n",
      "Epoch 70/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.7872\n",
      "Epoch 71/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.7564\n",
      "Epoch 72/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7140\n",
      "Epoch 73/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7592\n",
      "Epoch 74/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7288\n",
      "Epoch 75/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7266\n",
      "Epoch 76/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7096\n",
      "Epoch 77/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7013\n",
      "Epoch 78/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.7269\n",
      "Epoch 79/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.6434\n",
      "Epoch 80/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.6503\n",
      "Epoch 81/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.6472\n",
      "Epoch 82/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.6626\n",
      "Epoch 83/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.6385\n",
      "Epoch 84/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.6071\n",
      "Epoch 85/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.6200\n",
      "Epoch 86/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.6128\n",
      "Epoch 87/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.6116\n",
      "Epoch 88/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5993\n",
      "Epoch 89/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5794\n",
      "Epoch 90/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5859\n",
      "Epoch 91/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.5580\n",
      "Epoch 92/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.5660\n",
      "Epoch 93/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.5624\n",
      "Epoch 94/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.5291\n",
      "Epoch 95/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5384\n",
      "Epoch 96/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5396\n",
      "Epoch 97/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5044\n",
      "Epoch 98/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.5139\n",
      "Epoch 99/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4937\n",
      "Epoch 100/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4953\n",
      "Epoch 101/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4987\n",
      "Epoch 102/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4873\n",
      "Epoch 103/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4648\n",
      "Epoch 104/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4770\n",
      "Epoch 105/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4643\n",
      "Epoch 106/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4418\n",
      "Epoch 107/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4734\n",
      "Epoch 108/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4619\n",
      "Epoch 109/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4515\n",
      "Epoch 110/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4596\n",
      "Epoch 111/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4017\n",
      "Epoch 112/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4139\n",
      "Epoch 113/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3832\n",
      "Epoch 114/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.4118\n",
      "Epoch 115/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4026\n",
      "Epoch 116/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3858\n",
      "Epoch 117/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.4057\n",
      "Epoch 118/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3938\n",
      "Epoch 119/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3795\n",
      "Epoch 120/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3666\n",
      "Epoch 121/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3655\n",
      "Epoch 122/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3680\n",
      "Epoch 123/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3515\n",
      "Epoch 124/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3509\n",
      "Epoch 125/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.2951\n",
      "Epoch 126/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3395\n",
      "Epoch 127/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.2958\n",
      "Epoch 128/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.3041\n",
      "Epoch 129/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.3089\n",
      "Epoch 130/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.2670\n",
      "Epoch 131/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.2691\n",
      "Epoch 132/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.2979\n",
      "Epoch 133/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.2484\n",
      "Epoch 134/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.2560\n",
      "Epoch 135/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.2354\n",
      "Epoch 136/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.2416\n",
      "Epoch 137/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.2076\n",
      "Epoch 138/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.2116\n",
      "Epoch 139/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.1853\n",
      "Epoch 140/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 28.1488\n",
      "Epoch 141/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.1196\n",
      "Epoch 142/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.0772\n",
      "Epoch 143/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 28.0132\n",
      "Epoch 144/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.9773\n",
      "Epoch 145/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.9679\n",
      "Epoch 146/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.9181\n",
      "Epoch 147/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.8966\n",
      "Epoch 148/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.8470\n",
      "Epoch 149/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.8379\n",
      "Epoch 150/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.7838\n",
      "Epoch 151/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.7353\n",
      "Epoch 152/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.7335\n",
      "Epoch 153/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.7016\n",
      "Epoch 154/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.7038\n",
      "Epoch 155/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.6894\n",
      "Epoch 156/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.6176\n",
      "Epoch 157/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.5837\n",
      "Epoch 158/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.6401\n",
      "Epoch 159/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.5995\n",
      "Epoch 160/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.5466\n",
      "Epoch 161/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.5427\n",
      "Epoch 162/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.5000\n",
      "Epoch 163/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.4638\n",
      "Epoch 164/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.4026\n",
      "Epoch 165/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.4348\n",
      "Epoch 166/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.3624\n",
      "Epoch 167/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.3299\n",
      "Epoch 168/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.3129\n",
      "Epoch 169/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.3077\n",
      "Epoch 170/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.2605\n",
      "Epoch 171/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.2456\n",
      "Epoch 172/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.1892\n",
      "Epoch 173/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.1943\n",
      "Epoch 174/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.2025\n",
      "Epoch 175/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.1381\n",
      "Epoch 176/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.1887\n",
      "Epoch 177/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.1370\n",
      "Epoch 178/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.1391\n",
      "Epoch 179/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.0957\n",
      "Epoch 180/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.1343\n",
      "Epoch 181/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.1279\n",
      "Epoch 182/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0705\n",
      "Epoch 183/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0910\n",
      "Epoch 184/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0573\n",
      "Epoch 185/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0702\n",
      "Epoch 186/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0796\n",
      "Epoch 187/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0507\n",
      "Epoch 188/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.0240\n",
      "Epoch 189/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.0554\n",
      "Epoch 190/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0349\n",
      "Epoch 191/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0447\n",
      "Epoch 192/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.0055\n",
      "Epoch 193/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0488\n",
      "Epoch 194/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 27.0009\n",
      "Epoch 195/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 27.0368\n",
      "Epoch 196/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.9971\n",
      "Epoch 197/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.9735\n",
      "Epoch 198/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9874\n",
      "Epoch 199/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.9642\n",
      "Epoch 200/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9977\n",
      "Epoch 201/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.9780\n",
      "Epoch 202/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9764\n",
      "Epoch 203/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9651\n",
      "Epoch 204/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9252\n",
      "Epoch 205/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9371\n",
      "Epoch 206/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.9064\n",
      "Epoch 207/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.8861\n",
      "Epoch 208/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.9053\n",
      "Epoch 209/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.8503\n",
      "Epoch 210/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.8167\n",
      "Epoch 211/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.8255\n",
      "Epoch 212/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.8073\n",
      "Epoch 213/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.7964\n",
      "Epoch 214/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.8268\n",
      "Epoch 215/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7868\n",
      "Epoch 216/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7531\n",
      "Epoch 217/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.7589\n",
      "Epoch 218/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.7580\n",
      "Epoch 219/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7287\n",
      "Epoch 220/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7474\n",
      "Epoch 221/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7542\n",
      "Epoch 222/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6936\n",
      "Epoch 223/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6991\n",
      "Epoch 224/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.7459\n",
      "Epoch 225/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.7105\n",
      "Epoch 226/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6931\n",
      "Epoch 227/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6939\n",
      "Epoch 228/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7046\n",
      "Epoch 229/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.6831\n",
      "Epoch 230/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6825\n",
      "Epoch 231/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6858\n",
      "Epoch 232/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6799\n",
      "Epoch 233/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7033\n",
      "Epoch 234/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7162\n",
      "Epoch 235/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.7173\n",
      "Epoch 236/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6905\n",
      "Epoch 237/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6643\n",
      "Epoch 238/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6666\n",
      "Epoch 239/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6806\n",
      "Epoch 240/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.6650\n",
      "Epoch 241/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6524\n",
      "Epoch 242/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6668\n",
      "Epoch 243/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 26.6624\n",
      "Epoch 244/1000\n",
      "760/760 [==============================] - 4s 5ms/step - loss: 26.6391\n",
      "Epoch 245/1000\n",
      "760/760 [==============================] - 3s 5ms/step - loss: 26.6497\n",
      "Epoch 246/1000\n",
      "760/760 [==============================] - 3s 5ms/step - loss: 26.6724\n",
      "Epoch 247/1000\n",
      "760/760 [==============================] - 4s 5ms/step - loss: 26.6274\n",
      "Epoch 248/1000\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 26.6340\n",
      "Epoch 249/1000\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 26.6309\n",
      "Epoch 250/1000\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 26.6251\n",
      "Epoch 251/1000\n",
      "760/760 [==============================] - 4s 5ms/step - loss: 26.6341\n",
      "Epoch 252/1000\n",
      "760/760 [==============================] - 4s 5ms/step - loss: 26.6259\n",
      "Epoch 253/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6410\n",
      "Epoch 254/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5898\n",
      "Epoch 255/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6363\n",
      "Epoch 256/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.6042\n",
      "Epoch 257/1000\n",
      "760/760 [==============================] - 4s 5ms/step - loss: 26.6339\n",
      "Epoch 258/1000\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 26.5869\n",
      "Epoch 259/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6190\n",
      "Epoch 260/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6163\n",
      "Epoch 261/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.6008\n",
      "Epoch 262/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5964\n",
      "Epoch 263/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5859\n",
      "Epoch 264/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5714\n",
      "Epoch 265/1000\n",
      "760/760 [==============================] - 4s 5ms/step - loss: 26.5522\n",
      "Epoch 266/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 26.5734\n",
      "Epoch 267/1000\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 26.5900\n",
      "Epoch 268/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5752\n",
      "Epoch 269/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5426\n",
      "Epoch 270/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5905\n",
      "Epoch 271/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5724\n",
      "Epoch 272/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5941\n",
      "Epoch 273/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5576\n",
      "Epoch 274/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5713\n",
      "Epoch 275/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5435\n",
      "Epoch 276/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5726\n",
      "Epoch 277/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.6002\n",
      "Epoch 278/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5298\n",
      "Epoch 279/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5482\n",
      "Epoch 280/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5473\n",
      "Epoch 281/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5349\n",
      "Epoch 282/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5486\n",
      "Epoch 283/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5045\n",
      "Epoch 284/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5449\n",
      "Epoch 285/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5876\n",
      "Epoch 286/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5388\n",
      "Epoch 287/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5408\n",
      "Epoch 288/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5445\n",
      "Epoch 289/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5327\n",
      "Epoch 290/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5271\n",
      "Epoch 291/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5102\n",
      "Epoch 292/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5373\n",
      "Epoch 293/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5249\n",
      "Epoch 294/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5105\n",
      "Epoch 295/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5027\n",
      "Epoch 296/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5217\n",
      "Epoch 297/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5162\n",
      "Epoch 298/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5232\n",
      "Epoch 299/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5405\n",
      "Epoch 300/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5142\n",
      "Epoch 301/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5132\n",
      "Epoch 302/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 26.4962\n",
      "Epoch 303/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5216\n",
      "Epoch 304/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4996\n",
      "Epoch 305/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4992\n",
      "Epoch 306/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5104\n",
      "Epoch 307/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5149\n",
      "Epoch 308/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5125\n",
      "Epoch 309/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5205\n",
      "Epoch 310/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4937\n",
      "Epoch 311/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4913\n",
      "Epoch 312/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5103\n",
      "Epoch 313/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5225\n",
      "Epoch 314/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5050\n",
      "Epoch 315/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.5163\n",
      "Epoch 316/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4778\n",
      "Epoch 317/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5094\n",
      "Epoch 318/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5124\n",
      "Epoch 319/1000\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 26.4530\n",
      "Epoch 320/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4635\n",
      "Epoch 321/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4759\n",
      "Epoch 322/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.5013\n",
      "Epoch 323/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4816\n",
      "Epoch 324/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4965\n",
      "Epoch 325/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4595\n",
      "Epoch 326/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4477\n",
      "Epoch 327/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4350\n",
      "Epoch 328/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4516\n",
      "Epoch 329/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4527\n",
      "Epoch 330/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4545\n",
      "Epoch 331/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4450\n",
      "Epoch 332/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4723\n",
      "Epoch 333/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4232\n",
      "Epoch 334/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4522\n",
      "Epoch 335/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4291\n",
      "Epoch 336/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 26.4285\n",
      "Epoch 337/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4356\n",
      "Epoch 338/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4279\n",
      "Epoch 339/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4437\n",
      "Epoch 340/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4668\n",
      "Epoch 341/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4292\n",
      "Epoch 342/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4108\n",
      "Epoch 343/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4373\n",
      "Epoch 344/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4837\n",
      "Epoch 345/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4057\n",
      "Epoch 346/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4526\n",
      "Epoch 347/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4235\n",
      "Epoch 348/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4433\n",
      "Epoch 349/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3831\n",
      "Epoch 350/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4484\n",
      "Epoch 351/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3984\n",
      "Epoch 352/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3893\n",
      "Epoch 353/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4073\n",
      "Epoch 354/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4164\n",
      "Epoch 355/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3992\n",
      "Epoch 356/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4078\n",
      "Epoch 357/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 26.4211\n",
      "Epoch 358/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3900\n",
      "Epoch 359/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4004\n",
      "Epoch 360/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4412\n",
      "Epoch 361/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4118\n",
      "Epoch 362/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3841\n",
      "Epoch 363/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3856\n",
      "Epoch 364/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.4313\n",
      "Epoch 365/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3893\n",
      "Epoch 366/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3839\n",
      "Epoch 367/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3686\n",
      "Epoch 368/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3867\n",
      "Epoch 369/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.4097\n",
      "Epoch 370/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3867\n",
      "Epoch 371/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3742\n",
      "Epoch 372/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.3812\n",
      "Epoch 373/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3284\n",
      "Epoch 374/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3815\n",
      "Epoch 375/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.3188\n",
      "Epoch 376/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3497\n",
      "Epoch 377/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3582\n",
      "Epoch 378/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3433\n",
      "Epoch 379/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3389\n",
      "Epoch 380/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3389\n",
      "Epoch 381/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3470\n",
      "Epoch 382/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3458\n",
      "Epoch 383/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3214\n",
      "Epoch 384/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3108\n",
      "Epoch 385/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3094\n",
      "Epoch 386/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3753\n",
      "Epoch 387/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2893\n",
      "Epoch 388/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3355\n",
      "Epoch 389/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.3488\n",
      "Epoch 390/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2885\n",
      "Epoch 391/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2958\n",
      "Epoch 392/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2979\n",
      "Epoch 393/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.3276\n",
      "Epoch 394/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.3136\n",
      "Epoch 395/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2963\n",
      "Epoch 396/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2752\n",
      "Epoch 397/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2787\n",
      "Epoch 398/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2980\n",
      "Epoch 399/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2635\n",
      "Epoch 400/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2925\n",
      "Epoch 401/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2842\n",
      "Epoch 402/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2455\n",
      "Epoch 403/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2709\n",
      "Epoch 404/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2593\n",
      "Epoch 405/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2867\n",
      "Epoch 406/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2681\n",
      "Epoch 407/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2400\n",
      "Epoch 408/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2626\n",
      "Epoch 409/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2548\n",
      "Epoch 410/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2580\n",
      "Epoch 411/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2573\n",
      "Epoch 412/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2572\n",
      "Epoch 413/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2642\n",
      "Epoch 414/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2540\n",
      "Epoch 415/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2746\n",
      "Epoch 416/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2323\n",
      "Epoch 417/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2626\n",
      "Epoch 418/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2641\n",
      "Epoch 419/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2209\n",
      "Epoch 420/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2590\n",
      "Epoch 421/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2480\n",
      "Epoch 422/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2773\n",
      "Epoch 423/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2371\n",
      "Epoch 424/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2331\n",
      "Epoch 425/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2397\n",
      "Epoch 426/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2498\n",
      "Epoch 427/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2344\n",
      "Epoch 428/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2496\n",
      "Epoch 429/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2114\n",
      "Epoch 430/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2572\n",
      "Epoch 431/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2111\n",
      "Epoch 432/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2314\n",
      "Epoch 433/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2258\n",
      "Epoch 434/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 26.2208\n",
      "Epoch 435/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2500\n",
      "Epoch 436/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2504\n",
      "Epoch 437/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1991\n",
      "Epoch 438/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1881\n",
      "Epoch 439/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.2306\n",
      "Epoch 440/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1853\n",
      "Epoch 441/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1888\n",
      "Epoch 442/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1893\n",
      "Epoch 443/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1937\n",
      "Epoch 444/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1868\n",
      "Epoch 445/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1953\n",
      "Epoch 446/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1625\n",
      "Epoch 447/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1720\n",
      "Epoch 448/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1343\n",
      "Epoch 449/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1206\n",
      "Epoch 450/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1353\n",
      "Epoch 451/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1174\n",
      "Epoch 452/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1238\n",
      "Epoch 453/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1613\n",
      "Epoch 454/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1137\n",
      "Epoch 455/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1129\n",
      "Epoch 456/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.1310\n",
      "Epoch 457/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0697\n",
      "Epoch 458/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0720\n",
      "Epoch 459/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0857\n",
      "Epoch 460/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0546\n",
      "Epoch 461/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0397\n",
      "Epoch 462/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0492\n",
      "Epoch 463/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0355\n",
      "Epoch 464/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0229\n",
      "Epoch 465/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0224\n",
      "Epoch 466/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 26.0323\n",
      "Epoch 467/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9998\n",
      "Epoch 468/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9744\n",
      "Epoch 469/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 25.9376\n",
      "Epoch 470/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9791\n",
      "Epoch 471/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9816\n",
      "Epoch 472/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9361\n",
      "Epoch 473/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9758\n",
      "Epoch 474/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9390\n",
      "Epoch 475/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9501\n",
      "Epoch 476/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9322\n",
      "Epoch 477/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9147\n",
      "Epoch 478/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9419\n",
      "Epoch 479/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9356\n",
      "Epoch 480/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8957\n",
      "Epoch 481/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9086\n",
      "Epoch 482/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.9206\n",
      "Epoch 483/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8615\n",
      "Epoch 484/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8733\n",
      "Epoch 485/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8541\n",
      "Epoch 486/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8695\n",
      "Epoch 487/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8582\n",
      "Epoch 488/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8585\n",
      "Epoch 489/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8720\n",
      "Epoch 490/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8621\n",
      "Epoch 491/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8853\n",
      "Epoch 492/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 25.8733\n",
      "Epoch 493/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8376\n",
      "Epoch 494/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8173\n",
      "Epoch 495/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8187\n",
      "Epoch 496/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8613\n",
      "Epoch 497/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7994\n",
      "Epoch 498/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8332\n",
      "Epoch 499/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8429\n",
      "Epoch 500/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8661\n",
      "Epoch 501/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8182\n",
      "Epoch 502/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8210\n",
      "Epoch 503/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7966\n",
      "Epoch 504/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8194\n",
      "Epoch 505/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8218\n",
      "Epoch 506/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8191\n",
      "Epoch 507/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7967\n",
      "Epoch 508/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8464\n",
      "Epoch 509/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8107\n",
      "Epoch 510/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7804\n",
      "Epoch 511/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7911\n",
      "Epoch 512/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7947\n",
      "Epoch 513/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8158\n",
      "Epoch 514/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7824\n",
      "Epoch 515/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8275\n",
      "Epoch 516/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7871\n",
      "Epoch 517/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8017\n",
      "Epoch 518/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7520\n",
      "Epoch 519/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7690\n",
      "Epoch 520/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7595\n",
      "Epoch 521/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8206\n",
      "Epoch 522/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7780\n",
      "Epoch 523/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7548\n",
      "Epoch 524/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.8034\n",
      "Epoch 525/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7689\n",
      "Epoch 526/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7215\n",
      "Epoch 527/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7720\n",
      "Epoch 528/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7390\n",
      "Epoch 529/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7558\n",
      "Epoch 530/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7556\n",
      "Epoch 531/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7182\n",
      "Epoch 532/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7387\n",
      "Epoch 533/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6915\n",
      "Epoch 534/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7623\n",
      "Epoch 535/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7045\n",
      "Epoch 536/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7102\n",
      "Epoch 537/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6819\n",
      "Epoch 538/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7119\n",
      "Epoch 539/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7061\n",
      "Epoch 540/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6965\n",
      "Epoch 541/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6748\n",
      "Epoch 542/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6954\n",
      "Epoch 543/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7106\n",
      "Epoch 544/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6788\n",
      "Epoch 545/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6930\n",
      "Epoch 546/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6701\n",
      "Epoch 547/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6975\n",
      "Epoch 548/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6591\n",
      "Epoch 549/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6994\n",
      "Epoch 550/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.7038\n",
      "Epoch 551/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6927\n",
      "Epoch 552/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6931\n",
      "Epoch 553/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.6366\n",
      "Epoch 554/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6632\n",
      "Epoch 555/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6465\n",
      "Epoch 556/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6774\n",
      "Epoch 557/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6971\n",
      "Epoch 558/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6831\n",
      "Epoch 559/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6670\n",
      "Epoch 560/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6765\n",
      "Epoch 561/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6229\n",
      "Epoch 562/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6292\n",
      "Epoch 563/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6370\n",
      "Epoch 564/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.6688\n",
      "Epoch 565/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.6646\n",
      "Epoch 566/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6549\n",
      "Epoch 567/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6426\n",
      "Epoch 568/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6785\n",
      "Epoch 569/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6793\n",
      "Epoch 570/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6538\n",
      "Epoch 571/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6434\n",
      "Epoch 572/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6414\n",
      "Epoch 573/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6385\n",
      "Epoch 574/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6898\n",
      "Epoch 575/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6224\n",
      "Epoch 576/1000\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 25.6166\n",
      "Epoch 577/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6010\n",
      "Epoch 578/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6355\n",
      "Epoch 579/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6855\n",
      "Epoch 580/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6169\n",
      "Epoch 581/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6241\n",
      "Epoch 582/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6369\n",
      "Epoch 583/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6059\n",
      "Epoch 584/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6540\n",
      "Epoch 585/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6341\n",
      "Epoch 586/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6379\n",
      "Epoch 587/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6264\n",
      "Epoch 588/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6310\n",
      "Epoch 589/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6147\n",
      "Epoch 590/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6001\n",
      "Epoch 591/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5897\n",
      "Epoch 592/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5994\n",
      "Epoch 593/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6352\n",
      "Epoch 594/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5884\n",
      "Epoch 595/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5962\n",
      "Epoch 596/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6526\n",
      "Epoch 597/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6137\n",
      "Epoch 598/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5890\n",
      "Epoch 599/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6057\n",
      "Epoch 600/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6052\n",
      "Epoch 601/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5811\n",
      "Epoch 602/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5972\n",
      "Epoch 603/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5792\n",
      "Epoch 604/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6031\n",
      "Epoch 605/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5717\n",
      "Epoch 606/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5724\n",
      "Epoch 607/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6161\n",
      "Epoch 608/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6282\n",
      "Epoch 609/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5812\n",
      "Epoch 610/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6394\n",
      "Epoch 611/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5650\n",
      "Epoch 612/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5757\n",
      "Epoch 613/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5903\n",
      "Epoch 614/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6257\n",
      "Epoch 615/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5626\n",
      "Epoch 616/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5427\n",
      "Epoch 617/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5782\n",
      "Epoch 618/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6098\n",
      "Epoch 619/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5623\n",
      "Epoch 620/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5489\n",
      "Epoch 621/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5837\n",
      "Epoch 622/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5555\n",
      "Epoch 623/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5897\n",
      "Epoch 624/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.6093\n",
      "Epoch 625/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5496\n",
      "Epoch 626/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5452\n",
      "Epoch 627/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5561\n",
      "Epoch 628/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5693\n",
      "Epoch 629/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5993\n",
      "Epoch 630/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5669\n",
      "Epoch 631/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5746\n",
      "Epoch 632/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5745\n",
      "Epoch 633/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5685\n",
      "Epoch 634/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5435\n",
      "Epoch 635/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5549\n",
      "Epoch 636/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5747\n",
      "Epoch 637/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5603\n",
      "Epoch 638/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5482\n",
      "Epoch 639/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5403\n",
      "Epoch 640/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5503\n",
      "Epoch 641/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5526\n",
      "Epoch 642/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5844\n",
      "Epoch 643/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5453\n",
      "Epoch 644/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5939\n",
      "Epoch 645/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5201\n",
      "Epoch 646/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5230\n",
      "Epoch 647/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5520\n",
      "Epoch 648/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5450\n",
      "Epoch 649/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5298\n",
      "Epoch 650/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5375\n",
      "Epoch 651/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5486\n",
      "Epoch 652/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5221\n",
      "Epoch 653/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5431\n",
      "Epoch 654/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5130\n",
      "Epoch 655/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5068\n",
      "Epoch 656/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5615\n",
      "Epoch 657/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5281\n",
      "Epoch 658/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5267\n",
      "Epoch 659/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5690\n",
      "Epoch 660/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5565\n",
      "Epoch 661/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5471\n",
      "Epoch 662/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5333\n",
      "Epoch 663/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5147\n",
      "Epoch 664/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5475\n",
      "Epoch 665/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5231\n",
      "Epoch 666/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5555\n",
      "Epoch 667/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5082\n",
      "Epoch 668/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5293\n",
      "Epoch 669/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5424\n",
      "Epoch 670/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5482\n",
      "Epoch 671/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5227\n",
      "Epoch 672/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5287\n",
      "Epoch 673/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5163\n",
      "Epoch 674/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4439\n",
      "Epoch 675/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5271\n",
      "Epoch 676/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5148\n",
      "Epoch 677/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4909\n",
      "Epoch 678/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5112\n",
      "Epoch 679/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5043\n",
      "Epoch 680/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5054\n",
      "Epoch 681/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5337\n",
      "Epoch 682/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4832\n",
      "Epoch 683/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4803\n",
      "Epoch 684/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5007\n",
      "Epoch 685/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5070\n",
      "Epoch 686/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5079\n",
      "Epoch 687/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5244\n",
      "Epoch 688/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5701\n",
      "Epoch 689/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4607\n",
      "Epoch 690/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4667\n",
      "Epoch 691/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5015\n",
      "Epoch 692/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4884\n",
      "Epoch 693/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4863\n",
      "Epoch 694/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4907\n",
      "Epoch 695/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5069\n",
      "Epoch 696/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5021\n",
      "Epoch 697/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4617\n",
      "Epoch 698/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5297\n",
      "Epoch 699/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4785\n",
      "Epoch 700/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4863\n",
      "Epoch 701/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4781\n",
      "Epoch 702/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5035\n",
      "Epoch 703/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4397\n",
      "Epoch 704/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4604\n",
      "Epoch 705/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4507\n",
      "Epoch 706/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4503\n",
      "Epoch 707/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4918\n",
      "Epoch 708/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4960\n",
      "Epoch 709/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.4539\n",
      "Epoch 710/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4578\n",
      "Epoch 711/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4805\n",
      "Epoch 712/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4783\n",
      "Epoch 713/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4735\n",
      "Epoch 714/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5201\n",
      "Epoch 715/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4582\n",
      "Epoch 716/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4331\n",
      "Epoch 717/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4537\n",
      "Epoch 718/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4637\n",
      "Epoch 719/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4610\n",
      "Epoch 720/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4668\n",
      "Epoch 721/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4649\n",
      "Epoch 722/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4190\n",
      "Epoch 723/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4465\n",
      "Epoch 724/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4466\n",
      "Epoch 725/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.5006\n",
      "Epoch 726/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.4554\n",
      "Epoch 727/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.4550\n",
      "Epoch 728/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.4662\n",
      "Epoch 729/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4602\n",
      "Epoch 730/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4424\n",
      "Epoch 731/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4464\n",
      "Epoch 732/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4544\n",
      "Epoch 733/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.4700\n",
      "Epoch 734/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.4537\n",
      "Epoch 735/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4804\n",
      "Epoch 736/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4439\n",
      "Epoch 737/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.5063\n",
      "Epoch 738/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4476\n",
      "Epoch 739/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4497\n",
      "Epoch 740/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4244\n",
      "Epoch 741/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4731\n",
      "Epoch 742/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4745\n",
      "Epoch 743/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4209\n",
      "Epoch 744/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4345\n",
      "Epoch 745/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4216\n",
      "Epoch 746/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4210\n",
      "Epoch 747/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4639\n",
      "Epoch 748/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4353\n",
      "Epoch 749/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4636\n",
      "Epoch 750/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4440\n",
      "Epoch 751/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4358\n",
      "Epoch 752/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4359\n",
      "Epoch 753/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4325\n",
      "Epoch 754/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4477\n",
      "Epoch 755/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4489\n",
      "Epoch 756/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4256\n",
      "Epoch 757/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4130\n",
      "Epoch 758/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4645\n",
      "Epoch 759/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4270\n",
      "Epoch 760/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4496\n",
      "Epoch 761/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4295\n",
      "Epoch 762/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4530\n",
      "Epoch 763/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4277\n",
      "Epoch 764/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4159\n",
      "Epoch 765/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3928\n",
      "Epoch 766/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4495\n",
      "Epoch 767/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4059\n",
      "Epoch 768/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4188\n",
      "Epoch 769/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3626\n",
      "Epoch 770/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4519\n",
      "Epoch 771/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4354\n",
      "Epoch 772/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4116\n",
      "Epoch 773/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3915\n",
      "Epoch 774/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4197\n",
      "Epoch 775/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4309\n",
      "Epoch 776/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4252\n",
      "Epoch 777/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4151\n",
      "Epoch 778/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4175\n",
      "Epoch 779/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4281\n",
      "Epoch 780/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3932\n",
      "Epoch 781/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4304\n",
      "Epoch 782/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4273\n",
      "Epoch 783/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4578\n",
      "Epoch 784/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3873\n",
      "Epoch 785/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4253\n",
      "Epoch 786/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4302\n",
      "Epoch 787/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3925\n",
      "Epoch 788/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4395\n",
      "Epoch 789/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3982\n",
      "Epoch 790/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4307\n",
      "Epoch 791/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4423\n",
      "Epoch 792/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3881\n",
      "Epoch 793/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4155\n",
      "Epoch 794/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4464\n",
      "Epoch 795/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4173\n",
      "Epoch 796/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4040\n",
      "Epoch 797/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4074\n",
      "Epoch 798/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4024\n",
      "Epoch 799/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3809\n",
      "Epoch 800/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4013\n",
      "Epoch 801/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4182\n",
      "Epoch 802/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3922\n",
      "Epoch 803/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3937\n",
      "Epoch 804/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4109\n",
      "Epoch 805/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3989\n",
      "Epoch 806/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4338\n",
      "Epoch 807/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4224\n",
      "Epoch 808/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3984\n",
      "Epoch 809/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3759\n",
      "Epoch 810/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3963\n",
      "Epoch 811/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3974\n",
      "Epoch 812/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4056\n",
      "Epoch 813/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3770\n",
      "Epoch 814/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3697\n",
      "Epoch 815/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4049\n",
      "Epoch 816/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3782\n",
      "Epoch 817/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3890\n",
      "Epoch 818/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3850\n",
      "Epoch 819/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3908\n",
      "Epoch 820/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4046\n",
      "Epoch 821/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4150\n",
      "Epoch 822/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3629\n",
      "Epoch 823/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3841\n",
      "Epoch 824/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4137\n",
      "Epoch 825/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.3557\n",
      "Epoch 826/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3727\n",
      "Epoch 827/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3752\n",
      "Epoch 828/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3723\n",
      "Epoch 829/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3934\n",
      "Epoch 830/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4094\n",
      "Epoch 831/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3662\n",
      "Epoch 832/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3970\n",
      "Epoch 833/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3843\n",
      "Epoch 834/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3976\n",
      "Epoch 835/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3790\n",
      "Epoch 836/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3623\n",
      "Epoch 837/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3705\n",
      "Epoch 838/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4112\n",
      "Epoch 839/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3441\n",
      "Epoch 840/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3603\n",
      "Epoch 841/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3537\n",
      "Epoch 842/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3649\n",
      "Epoch 843/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3538\n",
      "Epoch 844/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3583\n",
      "Epoch 845/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3873\n",
      "Epoch 846/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3809\n",
      "Epoch 847/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3838\n",
      "Epoch 848/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3620\n",
      "Epoch 849/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3931\n",
      "Epoch 850/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3419\n",
      "Epoch 851/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.3843\n",
      "Epoch 852/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4053\n",
      "Epoch 853/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3976\n",
      "Epoch 854/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3630\n",
      "Epoch 855/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3481\n",
      "Epoch 856/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3689\n",
      "Epoch 857/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3749\n",
      "Epoch 858/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3499\n",
      "Epoch 859/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.4111\n",
      "Epoch 860/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3571\n",
      "Epoch 861/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3640\n",
      "Epoch 862/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3654\n",
      "Epoch 863/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3789\n",
      "Epoch 864/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3514\n",
      "Epoch 865/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.3685\n",
      "Epoch 866/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3370\n",
      "Epoch 867/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3318\n",
      "Epoch 868/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3634\n",
      "Epoch 869/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3904\n",
      "Epoch 870/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3952\n",
      "Epoch 871/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3307\n",
      "Epoch 872/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3813\n",
      "Epoch 873/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3760\n",
      "Epoch 874/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3575\n",
      "Epoch 875/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3680\n",
      "Epoch 876/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3623\n",
      "Epoch 877/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3329\n",
      "Epoch 878/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3783\n",
      "Epoch 879/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3570\n",
      "Epoch 880/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3439\n",
      "Epoch 881/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3776\n",
      "Epoch 882/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3842\n",
      "Epoch 883/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3266\n",
      "Epoch 884/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3908\n",
      "Epoch 885/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3612\n",
      "Epoch 886/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3302\n",
      "Epoch 887/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3567\n",
      "Epoch 888/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3609\n",
      "Epoch 889/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3571\n",
      "Epoch 890/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3241\n",
      "Epoch 891/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3770\n",
      "Epoch 892/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3396\n",
      "Epoch 893/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3621\n",
      "Epoch 894/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3828\n",
      "Epoch 895/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3342\n",
      "Epoch 896/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3589\n",
      "Epoch 897/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3230\n",
      "Epoch 898/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3419\n",
      "Epoch 899/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3309\n",
      "Epoch 900/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3486\n",
      "Epoch 901/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3221\n",
      "Epoch 902/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3135\n",
      "Epoch 903/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3328\n",
      "Epoch 904/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3340\n",
      "Epoch 905/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3203\n",
      "Epoch 906/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3251\n",
      "Epoch 907/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3501\n",
      "Epoch 908/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2995\n",
      "Epoch 909/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3067\n",
      "Epoch 910/1000\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 25.3273\n",
      "Epoch 911/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3074\n",
      "Epoch 912/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3266\n",
      "Epoch 913/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3172\n",
      "Epoch 914/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3131\n",
      "Epoch 915/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3538\n",
      "Epoch 916/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3198\n",
      "Epoch 917/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3620\n",
      "Epoch 918/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3198\n",
      "Epoch 919/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3441\n",
      "Epoch 920/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3120\n",
      "Epoch 921/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3333\n",
      "Epoch 922/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3214\n",
      "Epoch 923/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3479\n",
      "Epoch 924/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3236\n",
      "Epoch 925/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3191\n",
      "Epoch 926/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3305\n",
      "Epoch 927/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3146\n",
      "Epoch 928/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3031\n",
      "Epoch 929/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3197\n",
      "Epoch 930/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3419\n",
      "Epoch 931/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3211\n",
      "Epoch 932/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3237\n",
      "Epoch 933/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3068\n",
      "Epoch 934/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3133\n",
      "Epoch 935/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2989\n",
      "Epoch 936/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3536\n",
      "Epoch 937/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3017\n",
      "Epoch 938/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3015\n",
      "Epoch 939/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3238\n",
      "Epoch 940/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3192\n",
      "Epoch 941/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3461\n",
      "Epoch 942/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3150\n",
      "Epoch 943/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3322\n",
      "Epoch 944/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3487\n",
      "Epoch 945/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2658\n",
      "Epoch 946/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3314\n",
      "Epoch 947/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3233\n",
      "Epoch 948/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3047\n",
      "Epoch 949/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3126\n",
      "Epoch 950/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3110\n",
      "Epoch 951/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3073\n",
      "Epoch 952/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2973\n",
      "Epoch 953/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3390\n",
      "Epoch 954/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2929\n",
      "Epoch 955/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3144\n",
      "Epoch 956/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3684\n",
      "Epoch 957/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3164\n",
      "Epoch 958/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3284\n",
      "Epoch 959/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3236\n",
      "Epoch 960/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2969\n",
      "Epoch 961/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2835\n",
      "Epoch 962/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2980\n",
      "Epoch 963/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3316\n",
      "Epoch 964/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3257\n",
      "Epoch 965/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3027\n",
      "Epoch 966/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3173\n",
      "Epoch 967/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3366\n",
      "Epoch 968/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2955\n",
      "Epoch 969/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2813\n",
      "Epoch 970/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3128\n",
      "Epoch 971/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3095\n",
      "Epoch 972/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2843\n",
      "Epoch 973/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3021\n",
      "Epoch 974/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2985\n",
      "Epoch 975/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3559\n",
      "Epoch 976/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3085\n",
      "Epoch 977/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3349\n",
      "Epoch 978/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3134\n",
      "Epoch 979/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2628\n",
      "Epoch 980/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3117\n",
      "Epoch 981/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2944\n",
      "Epoch 982/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2938\n",
      "Epoch 983/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3156\n",
      "Epoch 984/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3399\n",
      "Epoch 985/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3157\n",
      "Epoch 986/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3446\n",
      "Epoch 987/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3003\n",
      "Epoch 988/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3032\n",
      "Epoch 989/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3069\n",
      "Epoch 990/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3015\n",
      "Epoch 991/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3073\n",
      "Epoch 992/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2955\n",
      "Epoch 993/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3295\n",
      "Epoch 994/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3212\n",
      "Epoch 995/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2782\n",
      "Epoch 996/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3138\n",
      "Epoch 997/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2888\n",
      "Epoch 998/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3123\n",
      "Epoch 999/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.2714\n",
      "Epoch 1000/1000\n",
      "760/760 [==============================] - 2s 2ms/step - loss: 25.3101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2758e34a90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the ANN model on the Training set\n",
    "ann.fit(X, Y, batch_size = 60, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOeJRZJSHSWx",
    "outputId": "9e23acc8-c281-4843-8c82-c97f67f6539c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.653473],\n",
       "       [29.102795],\n",
       "       [28.077515],\n",
       "       ...,\n",
       "       [22.010292],\n",
       "       [22.80846 ],\n",
       "       [24.18248 ]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_9 = ann.predict(X_test)\n",
    "y_pred_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IFw1alhdXzgg"
   },
   "outputs": [],
   "source": [
    "y_pred_9=y_pred_9.reshape(len(y_pred_9),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Qp32TtXFwCOj"
   },
   "outputs": [],
   "source": [
    "y=y_pred_9.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WNX4kgowI5J",
    "outputId": "db481c4a-76e5-4adc-bd1d-f8c602edfc23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11399"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "LYX2R1d2xHCj"
   },
   "outputs": [],
   "source": [
    "j=0\n",
    "a=[]\n",
    "for i in range(0,11399):\n",
    "    a.append(y[i][j])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QsfFNZdrUtOZ"
   },
   "outputs": [],
   "source": [
    "df_5=pd.DataFrame(list(zip(I,a)))\n",
    "df_5.columns=['ID','Time_taken (min)']\n",
    "df_5.to_csv('/content/ann_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYFnFMd0_eUv"
   },
   "source": [
    "#### -- Hyper Parameter Tuning of Ann with Grid Search Cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Hcsc37uuUtKQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Dlk-Z2QcH0C-"
   },
   "outputs": [],
   "source": [
    "# Hyper Parameter Tuning by Grid Search Cv\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(rate = 0.1))\n",
    "    classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dropout(rate = 0.1))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform'))\n",
    "    # load weights\n",
    "    # model.load_weights(\"weights.best.hdf5\")\n",
    "    classifier.compile(optimizer = optimizer, loss = 'mean_squared_error',metrics=['mean_squared_error'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhxJ_E88utvQ",
    "outputId": "7e38467d-670f-4a53-9563-a081e3dd8b7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# classifier= build_classifier()\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "classifier = KerasRegressor(build_fn = build_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "ftplvwj_Hz_X",
    "outputId": "4a4a2ef1-dc42-4839-ccde-82e818a52ee5"
   },
   "outputs": [],
   "source": [
    "parameters = {'batch_size': [20,40,80],\n",
    "              'epochs': [10,15,100],\n",
    "              'optimizer': ['adam', 'rmsprop']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           cv = 2)\n",
    "\n",
    "grid_search = grid_search.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVsQrAAfAttM"
   },
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters: \" + str(best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlIQI_boMqX8",
    "outputId": "1990ada7-ec3f-4ea7-b24b-f7d54ed348ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper Parameters - Losses & R2 Score\n",
    "\n",
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHCCqrItACc_"
   },
   "source": [
    "### 5. Compairing Results of all Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "ARlGRjn4MvCP",
    "outputId": "e6f0d17c-91c0-401b-de50-bfb77772ca02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KUNAL\\AnacondaN\\lib\\site-packages\\ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Using a string of single character colors as a color sequence is deprecated. Use an explicit list instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph for Compairing Test Accuracy btw Different Machine Learnn Algorithms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([1.5,1,2,1\n",
    "])#left,bootom ,width ,height                                                                                             #Add an axes to the figure.\n",
    "\n",
    "Models = ['Stacked_RF&XGBoost','Decision Tree','Bagged_Decision_tree','RandomForest','RandomForest_rand','XGBoost','Ann']     #X AXIS\n",
    "accuracy = [75.615,60.56,69.89,72.90,74.02,73,70.50]                         #Y AXIS\n",
    "ax.bar(Models,accuracy,color='rgkyc')        #rgkyc-different color\n",
    "plt.title(\"Test Accuracy\",fontsize=25)    #title label\n",
    "plt.xlabel(\"Models\",fontsize=15)    #X label\n",
    "plt.ylabel(\"Comparison of Test Accuracy \",fontsize=15)    #Y label\n",
    "plt.show()    #Display a figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BpC698kBTfr"
   },
   "source": [
    "### 6. Best Model with Highest R2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Stacked Random Forest Regressor & XG Boost Regressor: 75.615 ~ 76%</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank You!!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "history_visible": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
